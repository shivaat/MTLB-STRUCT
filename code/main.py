# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rVny2kSUrdL4VNudzYo2gQnj1QZ7fQ4A
"""

#from google.colab import drive
#drive.mount("/content/gdrive/", force_remount=True)

#!pip install transformers==2.9.1
#!pip install git+https://github.com/huggingface/transformers

#!pip install seqeval

#path = "/content/gdrive/My Drive/MWE2020/BERT-based"
#import os
#os.chdir(path)

import sys, os, subprocess
import logging
import json
import numpy as np

import torch
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, \
                              SequentialSampler
from torch.nn.utils.rnn import pad_sequence
from torch.nn import CrossEntropyLoss
from tqdm import trange, tqdm
from seqeval.metrics import accuracy_score, f1_score, classification_report

from transformers import BertTokenizer, AdamW 
#from pytorch_transformers import WarmupLinearSchedule
from transformers import get_linear_schedule_with_warmup
from transformers import BertForTokenClassification, BertModel

from torch_struct import DependencyCRF

#from corpus_reader import *
from preprocessing import DataProcessor, MweDataProcessor, NERDataSet
from evaluation import labels2Parsemetsv
from model import *
from berteval import *

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)
logger = logging.getLogger(__name__)

if len(sys.argv)>1:
    config_path = sys.argv[1]
else:
    print("Missing Argument: please specify the path to the config .json file!")
with open(config_path) as f:
    config = json.load(f)

DEVorTEST = config["mode"]  # 'DEV' or 'TEST'
LANG = config["data"]["language"]
MAX_LEN = config["data"]["max_len"]   
BATCH_SIZE = config["training"]["batch_size"]  
NUM_EPOCHS = config["training"]["num_epochs"]
PRETRAIN_MODEL = config["model"]["pretrained_model_name"]   
MULTI_TASK = config["model"]["multi_task"]

lr = config["training"]["learning_rate"]
if MULTI_TASK:
    dep_loss_factor = config["training"]["dep_loss_factor"]

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else 'cpu'
data_path = config["data"]["data_path"]+ config["data"]["language"]+"/"   
to_save_path = config["training"]["save_dir"]

print("device index", device_name)
config["training"]["gpu-device"] = device_name 

tokenizer = BertTokenizer.from_pretrained(config["model"]["pretrained_model_vocab"]) #, do_lower_case=True)

### LOADING DATA ###
data_processor = MweDataProcessor(data_path)

train_examples = data_processor.get_train_examples()
val_examples = data_processor.get_dev_examples()

# max_len based on val-examples in order to have proper evaluation
# and in case train has some not legitimate long sentences
max_len = max([len(tokenizer.tokenize(x.text)) for x in val_examples])
max_len = max_len+2
print('max len based on data', max_len)
config["data"]["max_len_based_on_data"] = max_len

if max_len> config["data"]["max_len"]:
    max_len = config["data"]["max_len"]
print('max len', max_len)

config["data"]["max_len"] = max_len

if DEVorTEST == 'TEST':
	train_examples += val_examples

test_examples = data_processor.get_test_examples()
print('train size:', len(train_examples), 'dev size:', len(val_examples), \
      'test size:', len(test_examples))

# we don't use pos and dependency relation labels for now!
poses_dict = data_processor.get_pos_dict()      
deprels_dict = data_processor.get_deprels_dict()

tags2idx = {}
if "transfer" not in config["model"] or not config["model"]["transfer"]:
    tags_vals = data_processor.get_labels()
    for (i, label) in enumerate(tags_vals):
        tags2idx[label] = i
#tags_vals = {"[CLS]": 0, "[SEP]": 1, "X": 2}
else:
    tags2idx = config["data"]["tags2idx"] 

idx2tags = {tags2idx[t]: t for t in tags2idx}
print('# of labels:',len(tags2idx))
print('tags2idx:', tags2idx)

config["data"]["tags2idx"] = tags2idx

train_dataset = NERDataSet(data_list=train_examples, tokenizer=tokenizer, 
                           label_map=tags2idx, pos_map=poses_dict, 
                           deptype_map=deprels_dict, max_len=max_len)
eval_dataset = NERDataSet(data_list=val_examples, tokenizer=tokenizer, 
                          label_map=tags2idx, pos_map=poses_dict, 
                          deptype_map=deprels_dict, max_len=max_len)

test_dataset = NERDataSet(data_list=test_examples, tokenizer=tokenizer, 
                          label_map=tags2idx, pos_map=poses_dict, 
                          deptype_map=deprels_dict, max_len=max_len)

train_iter = DataLoader(dataset=train_dataset,
                                 batch_size=BATCH_SIZE,
                                 shuffle=False, #True,
                                 num_workers=4)
eval_iter = DataLoader(dataset=eval_dataset,
                                batch_size=BATCH_SIZE,
                                shuffle=False,
                                num_workers=4)
test_iter = DataLoader(dataset=test_dataset,
                                batch_size=BATCH_SIZE,
                                shuffle=False,
                                num_workers=4)

###  CREATING AN OBJECT OF THE MODEL  ###
if not MULTI_TASK:
    model = CoNLLClassifier.from_pretrained(PRETRAIN_MODEL, 
                                        num_labels=len(tags2idx)).to(device)

else:
    model = DepMultiTaskClassify(PRETRAIN_MODEL, len(tags2idx)).to(device)

if "transfer" in config["model"] and config["model"]["transfer"]:
    with open(config["model"]["transfer"].split("dep_tagger.torch")[0]+"config_saved.json") as f:
        temp_tags = json.load(f)["data"]["tags2idx"]
    model.classifier = nn.Linear(768, len(temp_tags)).to(device)
    #model.classifier = nn.Linear(768, len(deprels_dict)).to(device)  #  This is for when we transfer from a model trained on depTypes 
    model.load_state_dict(torch.load(config["model"]["transfer"]))
    model.classifier = nn.Linear(768, len(tags2idx)).to(device)   #
    model.num_labels = len(tags2idx)   #
    model = model.eval()
    eval(eval_iter, model, tags2idx, device, MULTI_TASK)
    print("**** Out-of-the-box evaluation on the transfered model performed ****")

### SETTING OPTIMIZATION PARAMETERS FOR FINE_TUNING  ###
num_train_optimization_steps = int(len(train_examples) / BATCH_SIZE) * NUM_EPOCHS

FULL_FINETUNING = True
if FULL_FINETUNING:
        param_optimizer = list(model.named_parameters())
        no_decay = ['bias', 'gamma', 'beta']
        optimizer_grouped_parameters = [
            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
             'weight_decay_rate': 0.01},
            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
             'weight_decay_rate': 0.0}
        ]
else:
        param_optimizer = list(model.classifier.named_parameters())
        optimizer_grouped_parameters = [{"params": [p for n, p in param_optimizer]}]

warmup_steps = int(0.1 * num_train_optimization_steps)
optimizer = AdamW(optimizer_grouped_parameters, lr=lr)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,
                                     num_training_steps=num_train_optimization_steps)

best_epoch_number = -1
best_epoch_score = -1

if not os.path.isdir(to_save_path):
    os.mkdir(to_save_path)

to_save_path = to_save_path + LANG + '_'  + DEVorTEST + "_"+ PRETRAIN_MODEL.replace("/", "-")
if MULTI_TASK:
    to_save_path = to_save_path + "_multitask"
else:
    to_save_path = to_save_path + "_single" 

if "transfer" in config["model"] and config["model"]["transfer"]:
    to_save_path = to_save_path + "_transferred"

if not os.path.isdir(to_save_path):
    os.mkdir(to_save_path)

model_name = "tagger.torch"
config["model"]["saved_model_name"] = model_name


best_score = -1
best_epoch = -1

###   TRAINING   ###
max_grad_norm = 1.0
for epoch in trange(NUM_EPOCHS, desc="Epoch"):
    model = model.train()
    losses = []
    losses_main, losses_dep = [], []
    tr_loss = 0
    nb_tr_steps = 0
    for step, batch in enumerate(tqdm(train_iter)):
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_pos_ids, b_tags, b_deptype_ids, b_labels, b_input_mask,\
         b_token_type_ids, b_label_masks, lengths = batch
        
        lengths = torch.flatten(lengths)
        batch_size, _ = b_tags.shape

        if not MULTI_TASK:
            loss, logits, labels = model(b_input_ids, b_tags,  # b_tags is not used for now
                                     token_type_ids=b_token_type_ids,
                                     attention_mask=b_input_mask, 
                                     labels=b_labels,
                                     label_masks=b_label_masks)
            loss.backward()
            tr_loss += loss.item()
            nb_tr_steps += 1
            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), 
                                       max_norm=max_grad_norm)     
            optimizer.step()
            scheduler.step()

        else:
            b_tags = [tag[mask] for mask, tag in zip(b_label_masks, b_tags)]
            b_tags = pad_sequence(b_tags, batch_first=True, padding_value=0)

            loss_main, logits, labels, final = model(b_input_ids, b_tags, labels=b_labels, label_masks=b_label_masks)

            if not lengths.max() <= final.shape[1]:
                dep_loss = 0
            else:
                dist = DependencyCRF(final, lengths=lengths)
                dep_labels = dist.struct.to_parts(b_tags, lengths=lengths).type_as(final)   # [BATCH_SIZE, lengths, lengths]
                log_prob = dist.log_prob(dep_labels)

                dep_loss = log_prob.mean() #sum()

            if dep_loss < 0 :
                loss = loss_main -dep_loss/dep_loss_factor  
            else:
                loss = loss_main

            loss.backward()
            tr_loss += loss.item()
            nb_tr_steps += 1
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            optimizer.step()
            scheduler.step()
            losses.append(loss.detach())
            losses_main.append(loss_main.detach())
            if not lengths.max() <= final.shape[1]:
                losses_dep.append(dep_loss)
            else:
                losses_dep.append(dep_loss.detach())
    
            if step % 50 == 1:
                print("train loss:", torch.tensor(losses).mean(), 'main loss', torch.tensor(losses_main).mean(), 
                        'dep aux loss', torch.tensor(losses_dep).mean())
                losses = []


        model.zero_grad()

    logger.info("Train loss: {}".format(tr_loss / nb_tr_steps))
    if DEVorTEST == 'DEV':
        ls, ps, f1_score = eval(eval_iter, model, tags2idx, device, MULTI_TASK)
        if f1_score > best_score:
              best_score = f1_score
              best_epoch = epoch+1
              config["best_score"] = best_score
              config["best_epoch"] = best_epoch

              torch.save(model.state_dict(), to_save_path + '/tagger.torch')
              np.save(to_save_path +"/Idx2Tags.npy", idx2tags)
              json.dump(config, open(to_save_path+"/config_saved.json", 'w'))


model = model.eval()

if DEVorTEST == 'TEST':
    torch.save(model.state_dict(), to_save_path + '/tagger.torch')
    np.save(to_save_path +"/Idx2Tags.npy", idx2tags)
    json.dump(config, open(to_save_path+"/config_saved.json", 'w'))


prediction_file_name = to_save_path + '/bertTaggerResults_'+DEVorTEST
if DEVorTEST == 'TEST':
	labels, probs = eval_blind(test_iter, model, tags2idx, device, MULTI_TASK)
	labels2Parsemetsv(labels, data_path+'test.blind.cupt', prediction_file_name+'_system.cupt')
else:
	labels, probs, _ = eval(eval_iter, model, tags2idx, device, MULTI_TASK)
	labels2Parsemetsv(labels, data_path+'dev.cupt', prediction_file_name+'_system.cupt')

	
